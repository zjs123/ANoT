{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------#\n",
    "###生成实体类别###\n",
    "#--------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from prefixspan import PrefixSpan\n",
    "#import spacy\n",
    "#import en_core_web_trf\n",
    "#import spacy_transformers\n",
    "\n",
    "#nlp = spacy.load('en_core_web_trf')\n",
    "root_path = '/home/zhangjs/expriment/TKGist/data/'\n",
    "dataset = 'GDELT/'\n",
    "\n",
    "from itertools import combinations, chain\n",
    "\n",
    "def powerset(iterable, num):\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(1, num)))\n",
    "\n",
    "\n",
    "# read entity2id file\n",
    "raw_e2id_dict = {}\n",
    "raw_id2e_dict = {}\n",
    "file = open(root_path+dataset+'entity2id.txt', 'r')\n",
    "for e2id in file.readlines():\n",
    "    e, id = e2id.strip().split('\t')[:2]\n",
    "    raw_e2id_dict[e] = int(id)\n",
    "    raw_id2e_dict[int(id)] = e\n",
    "print(len(raw_e2id_dict))\n",
    "print(raw_id2e_dict[0])\n",
    "\n",
    "e_num = len(raw_e2id_dict)\n",
    "\n",
    "# read relation2id file\n",
    "raw_r2id_dict = {}\n",
    "raw_id2r_dict = {}\n",
    "file = open(root_path+dataset+'relation2id.txt', 'r')\n",
    "for r2id in file.readlines():\n",
    "    r, id = r2id.strip().split('\t')[:2]\n",
    "    raw_r2id_dict[r] = int(id)\n",
    "    raw_id2r_dict[int(id)] = r\n",
    "print(len(raw_r2id_dict))\n",
    "print(raw_id2r_dict[0])\n",
    "\n",
    "r_num = len(raw_r2id_dict)\n",
    "print(r_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(root_path+dataset+'train.txt', 'r')\n",
    "triple_2_t = dict()\n",
    "for fact in file.readlines():\n",
    "    s, r, o, t = fact.strip().split('\t')[:4]\n",
    "    s = int(s)\n",
    "    r = int(r)\n",
    "    o = int(o)\n",
    "    if (s,r,o) not in triple_2_t.keys():\n",
    "        triple_2_t[(s,r,o)] = []\n",
    "    triple_2_t[(s,r,o)].append(t)\n",
    "    if (o,r,s) not in triple_2_t.keys():\n",
    "        triple_2_t[(o,r,s)] = []\n",
    "    triple_2_t[(o,r,s)].append(t)\n",
    "\n",
    "print(len(triple_2_t))\n",
    "repeat_num = 0\n",
    "for key in triple_2_t.keys():\n",
    "    if len(triple_2_t[key]) > 1:\n",
    "        repeat_num += 1\n",
    "print(repeat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取原始关系序列\n",
    "import random\n",
    "# read relation sets (注：存在新实体)\n",
    "seen_e_set = set()\n",
    "seen_r_set = set()\n",
    "raw_e2rset_dict = {}\n",
    "raw_r_type_set = set()\n",
    "raw_r_type_list = []\n",
    "raw_e_2_type_list = []\n",
    "file = open(root_path+dataset+'train.txt', 'r')\n",
    "for fact in file.readlines():\n",
    "    s, r, o, t = fact.strip().split('\t')[:4]\n",
    "    s = int(s)\n",
    "    r = int(r)\n",
    "    o = int(o)\n",
    "    t = int(t)\n",
    "    \n",
    "    if s not in raw_e2rset_dict.keys():\n",
    "        raw_e2rset_dict[s] = set()\n",
    "    if o not in raw_e2rset_dict.keys():\n",
    "        raw_e2rset_dict[o] = set()\n",
    "    raw_e2rset_dict[s].add(r)\n",
    "    raw_e2rset_dict[o].add(r+r_num)\n",
    "\n",
    "    seen_e_set.add(s)\n",
    "    seen_e_set.add(o)\n",
    "    seen_r_set.add(r)\n",
    "    seen_r_set.add(r+r_num)\n",
    "\n",
    "for key in raw_e2rset_dict.keys():\n",
    "    raw_e2rset_dict[key] = sorted(raw_e2rset_dict[key])\n",
    "    raw_r_type_set.add(tuple(raw_e2rset_dict[key]))\n",
    "    raw_r_type_list.append(tuple(raw_e2rset_dict[key]))\n",
    "    raw_e_2_type_list.append(key)\n",
    "    \n",
    "print(len(seen_e_set))\n",
    "print(len(seen_r_set))\n",
    "print(len(raw_r_type_set))# 原始数据集中有多少种不同的关系组合\n",
    "print(raw_e2rset_dict)\n",
    "print(raw_r_type_set)\n",
    "print(seen_e_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取频繁关系子序列\n",
    "import numpy as np\n",
    "\n",
    "tmp_set = set()\n",
    "cover_e_set = set()\n",
    "cover_r_set = set()\n",
    "e_list = raw_e_2_type_list #list(raw_e2rset_dict.keys())\n",
    "sequence_list = raw_r_type_list #list(raw_e2rset_dict.values())\n",
    "match_num = [0]*len(sequence_list)\n",
    "tmp_freq_rset = []\n",
    "raw_data = []\n",
    "freq_rset = set()\n",
    "\n",
    "fail_num = 0\n",
    "patt_len = 3 # 3 for all datsets\n",
    "\n",
    "def cover_e(patt, matches):\n",
    "    cover_e = set()\n",
    "    for i in matches:\n",
    "        \n",
    "        if match_num[i[0]] < 5:\n",
    "            cover_e.add(e_list[i[0]])\n",
    "            match_num[i[0]] += 1\n",
    "        else:\n",
    "            cover_e.add(e_list[i[0]])\n",
    "            sequence_list[i[0]] = tuple([])\n",
    "            match_num[i[0]] += 1\n",
    "        \n",
    "    tmp_freq_rset.append((len(cover_e), tuple(sorted(patt)), tuple(sorted(cover_e))))\n",
    "\n",
    "while len(cover_e_set & seen_e_set) != len(seen_e_set):\n",
    "    tmp_freq_rset = []\n",
    "    pre_num_cover_e, pre_num_cover_r = len(cover_e_set), len(cover_r_set)\n",
    "\n",
    "    ps = PrefixSpan(sequence_list)\n",
    "    ps.topk(1000, closed = True, filter=lambda patt, matches: len(patt) > patt_len and tuple(sorted(patt)) not in freq_rset, callback=cover_e)\n",
    "    \n",
    "    tmp_num_cover_e = pre_num_cover_e\n",
    "    for rset in tmp_freq_rset:\n",
    "        if rset[1] not in freq_rset:\n",
    "            cover_r_set = cover_r_set | set(rset[1])\n",
    "            cover_e_set = cover_e_set | set(rset[2])\n",
    "            if len(cover_e_set) > tmp_num_cover_e:\n",
    "                print([len(cover_e_set), len(cover_r_set)])\n",
    "                tmp_num_cover_e = len(cover_e_set)\n",
    "                raw_data.append(rset)\n",
    "                freq_rset.add(rset[1])\n",
    "\n",
    "    \n",
    "    if pre_num_cover_e == len(cover_e_set):\n",
    "        fail_num += 1\n",
    "    else:\n",
    "        pre_num_cover_e = len(cover_e_set)\n",
    "    if fail_num == 3:\n",
    "        fail_num = 0\n",
    "        patt_len = patt_len - 1\n",
    "\n",
    "print(len(freq_rset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于集合包含关系合并频繁关系子序列\n",
    "\n",
    "# 基于类别的集合包含关系合并 O(N*2^k), 优化前为 O(N^2), 2^k << N\n",
    "sorted_freq_r_list = sorted(raw_data, key = lambda x:len(x[1]), reverse = True)\n",
    "\n",
    "sparse_type_set = set()\n",
    "childset_2_fatherset = {}\n",
    "cover_e_2_freq_r_dict = {}\n",
    "filtered_freq_r_2_cover_e_dict = {}\n",
    "for sub_r_set in tqdm(sorted_freq_r_list):\n",
    "    patt = tuple(sorted(sub_r_set[1]))\n",
    "    cover_e = sub_r_set[2]\n",
    "    if 'YAGO' in dataset:\n",
    "        filtered_freq_r_2_cover_e_dict[patt] = tuple(sorted(cover_e))\n",
    "        continue\n",
    "\n",
    "    if patt in childset_2_fatherset.keys():\n",
    "        fathersets = childset_2_fatherset[patt]\n",
    "        for father_set in fathersets:\n",
    "            filtered_freq_r_2_cover_e_dict[father_set] = tuple(sorted(set(filtered_freq_r_2_cover_e_dict[father_set]) | set(cover_e)))\n",
    "    \n",
    "    else:\n",
    "        if len(cover_e) >= 5:\n",
    "            filtered_freq_r_2_cover_e_dict[patt] = tuple(sorted(cover_e))\n",
    "            childsets = powerset(patt, len(patt)+1)\n",
    "            for child_set in childsets:\n",
    "                child_set = tuple(sorted(child_set))\n",
    "                if child_set not in childset_2_fatherset.keys():\n",
    "                    childset_2_fatherset[child_set] = []\n",
    "                childset_2_fatherset[child_set].append(patt)\n",
    "        else:\n",
    "            childsets = powerset(patt, len(patt))\n",
    "            flag = 0\n",
    "            for child_set in childsets:\n",
    "                child_set = tuple(sorted(child_set))\n",
    "                if child_set in childset_2_fatherset.keys():\n",
    "                    flag = 1\n",
    "                    fathersets = childset_2_fatherset[child_set]\n",
    "                    for father_set in fathersets:\n",
    "                        filtered_freq_r_2_cover_e_dict[father_set] = tuple(sorted(set(filtered_freq_r_2_cover_e_dict[father_set]) | set(cover_e)))\n",
    "            if flag == 0:\n",
    "                filtered_freq_r_2_cover_e_dict[patt] = tuple(sorted(cover_e))\n",
    "                if len(cover_e) == 1:\n",
    "                    sparse_type_set.add(patt)\n",
    "\n",
    "print(\"Number of sparse types: \" + str(len(sparse_type_set)))\n",
    "print(\"Sparse types: \" + str(sparse_type_set))\n",
    "\n",
    "for key in filtered_freq_r_2_cover_e_dict.keys():\n",
    "    value = tuple(sorted(filtered_freq_r_2_cover_e_dict[key]))\n",
    "    if value not in cover_e_2_freq_r_dict.keys():\n",
    "        cover_e_2_freq_r_dict[value] = tuple(sorted(key))\n",
    "    else:\n",
    "        cover_e_2_freq_r_dict[value] = tuple(sorted(cover_e_2_freq_r_dict[value] | set(key)))\n",
    "\n",
    "print(\"Number of types: \" + str(len(cover_e_2_freq_r_dict.keys())))\n",
    "\n",
    "if 'YAGO' not in dataset:\n",
    "    cover_e_set = set()\n",
    "    for key in cover_e_2_freq_r_dict.keys():\n",
    "        cover_e_set = cover_e_set | set(key)\n",
    "    print(len(cover_e_set & seen_e_set))\n",
    "\n",
    "    # 基于覆盖实体的集合包含关系合并, 优化前为 O(M^2)\n",
    "    del_num = 1\n",
    "    times = 0\n",
    "    while del_num != 0 and times < 5:\n",
    "        times += 1\n",
    "        sorted_cover_e_list = sorted(cover_e_2_freq_r_dict.items(), key = lambda x:len(x[0]), reverse = False)\n",
    "\n",
    "        del_set = set()\n",
    "        add_dict = {} #father_cover_e 2 (child_cover_e, r_of_child_cover_e)\n",
    "        for i in tqdm(range(len(sorted_cover_e_list) - 1)):\n",
    "            cover_e = sorted_cover_e_list[i][0]\n",
    "            patt = sorted_cover_e_list[i][1]\n",
    "            in_i = i+1\n",
    "            while in_i < len(sorted_cover_e_list):\n",
    "                in_cover_e = sorted_cover_e_list[in_i][0]\n",
    "                in_patt = sorted_cover_e_list[in_i][1]\n",
    "                in_i += 1\n",
    "\n",
    "                cover_ratio = float(len(set(cover_e) & set(in_cover_e))) / float(min(len(cover_e), len(in_cover_e)))\n",
    "                if cover_ratio >= 0.8: # 0.8\n",
    "                    if in_cover_e not in add_dict.keys():\n",
    "                        add_dict[in_cover_e] = []\n",
    "                    add_dict[in_cover_e].append([set(cover_e), set(patt)])\n",
    "                    del_set.add(cover_e)\n",
    "                else:\n",
    "                    if len(cover_e) < 5 and len(set(cover_e) - set(in_cover_e)) < len(cover_e):\n",
    "                        if in_cover_e not in add_dict.keys():\n",
    "                            add_dict[in_cover_e] = []\n",
    "                        add_dict[in_cover_e].append([set(cover_e), set(patt)])\n",
    "                        del_set.add(cover_e)\n",
    "\n",
    "        del_num = 0\n",
    "        cover_e_2_freq_r_dict = {}\n",
    "        for item in sorted_cover_e_list:\n",
    "            cover_e = item[0]\n",
    "            patt = item[1]\n",
    "\n",
    "            if cover_e in add_dict.keys():\n",
    "                new_cover_e = set(cover_e)\n",
    "                new_patt = set(patt)\n",
    "                for in_item in add_dict[cover_e]:\n",
    "                    new_cover_e = new_cover_e | in_item[0]\n",
    "                    new_patt = new_patt | in_item[1]\n",
    "                new_cover_e = tuple(sorted(new_cover_e))\n",
    "                new_patt = tuple(sorted(new_patt))\n",
    "                cover_e_2_freq_r_dict[new_cover_e] = new_patt\n",
    "            else:\n",
    "                if cover_e in del_set:\n",
    "                    del_num += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    cover_e_2_freq_r_dict[tuple(cover_e)] = tuple(patt)\n",
    "    \n",
    "        print(\"DEL \" + str(del_num) + \" types\")\n",
    "        cover_e_set = set()\n",
    "        for key in cover_e_2_freq_r_dict.keys():\n",
    "            cover_e_set = cover_e_set | set(key)\n",
    "        print([len(cover_e_set & seen_e_set), len(cover_e_set), len(seen_e_set)])\n",
    "                \n",
    "\n",
    "    # 获取类别—>覆盖实体 字典\n",
    "\n",
    "    filtered_freq_r_2_cover_e_dict = {}\n",
    "    for cover_e in cover_e_2_freq_r_dict.keys():\n",
    "        patt = cover_e_2_freq_r_dict[cover_e]\n",
    "        if patt not in filtered_freq_r_2_cover_e_dict.keys():\n",
    "            filtered_freq_r_2_cover_e_dict[patt] = cover_e\n",
    "        else:\n",
    "            filtered_freq_r_2_cover_e_dict[patt] = tuple(sorted(set(cover_e) | set(filtered_freq_r_2_cover_e_dict[patt])))\n",
    "\n",
    "print(\"Number of filtered types: \" + str(len(filtered_freq_r_2_cover_e_dict.keys())))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_freq_r_2_cover_e_dict)\n",
    "\n",
    "cover_e_set = set()\n",
    "for value in filtered_freq_r_2_cover_e_dict.values():\n",
    "    cover_e_set = cover_e_set | set(value)\n",
    "print(len(cover_e_set & seen_e_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于贪心算法求最小集合覆盖\n",
    "uncover_e_set = seen_e_set\n",
    "final_r_types = set()\n",
    "iter_num = 1\n",
    "\n",
    "while len(uncover_e_set) > 0:\n",
    "    print(\"iter\" + str(iter_num) + \"|\" + str(len(final_r_types)) + \"|\" + str(len(uncover_e_set)))\n",
    "    iter_num += 1\n",
    "    max_cover_num = 0\n",
    "    max_cover_type = None\n",
    "    max_cover_e = set()\n",
    "    for r_type in filtered_freq_r_2_cover_e_dict.keys():\n",
    "        inject_num = len(uncover_e_set & set(filtered_freq_r_2_cover_e_dict[r_type]))\n",
    "        if inject_num > max_cover_num:\n",
    "            max_cover_num = inject_num\n",
    "            max_cover_type = r_type\n",
    "            max_cover_e = set(filtered_freq_r_2_cover_e_dict[r_type])\n",
    "    if max_cover_num != 0:\n",
    "        uncover_e_set = uncover_e_set - max_cover_e\n",
    "        final_r_types.add(max_cover_type)\n",
    "\n",
    "print(\"Final number of types: \" + str(len(final_r_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过约束每个实体的最大类别数消减类别数量\n",
    "import numpy as np\n",
    "\n",
    "k = 5 #10 for ICEWS14 ,ICEWS05, YAGO\n",
    "e_2_type_dict = {}\n",
    "type_2_e_dict = {}\n",
    "\n",
    "for key in filtered_freq_r_2_cover_e_dict.keys():\n",
    "    for e in filtered_freq_r_2_cover_e_dict[key]:\n",
    "        if e not in e_2_type_dict.keys():\n",
    "            e_2_type_dict[e] = set()\n",
    "        e_2_type_dict[e].add(key)\n",
    "\n",
    "for e in e_2_type_dict.keys():\n",
    "    old_types = e_2_type_dict[e]\n",
    "    sorted_old_types = sorted(old_types, key=lambda type: len(filtered_freq_r_2_cover_e_dict[type]), reverse=True)\n",
    "    new_types = sorted_old_types[:k]\n",
    "    e_2_type_dict[e] = new_types\n",
    "    for type in new_types:\n",
    "        if type not in type_2_e_dict.keys():\n",
    "            type_2_e_dict[type] = set()\n",
    "        type_2_e_dict[type].add(e)\n",
    "\n",
    "r_2_cover_e_num = []\n",
    "e_2_type_num = []\n",
    "for key in type_2_e_dict.keys():\n",
    "    r_2_cover_e_num.append(len(type_2_e_dict[key]))\n",
    "for key in e_2_type_dict.keys():\n",
    "    e_2_type_num.append(len(e_2_type_dict[key]))\n",
    "\n",
    "print(len(type_2_e_dict.keys()))\n",
    "\n",
    "# 关系的覆盖实体数量统计\n",
    "print([min(r_2_cover_e_num), np.mean(r_2_cover_e_num), max(r_2_cover_e_num)])\n",
    "\n",
    "# 实体的关系数量统计\n",
    "print([min(e_2_type_num), np.mean(e_2_type_num), max(e_2_type_num)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type_2_e_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储类别文件\n",
    "import numpy as np\n",
    "\n",
    "type_2_id = dict()\n",
    "id = 0\n",
    "\n",
    "f = open(root_path+dataset+'types.txt','w', encoding='utf-8')\n",
    "for type in type_2_e_dict.keys():\n",
    "    type_2_id[type] = id\n",
    "    f.writelines(str(type) + '\t' + str(id) + '\\n')\n",
    "    id += 1\n",
    "f.close()\n",
    "\n",
    "e_2_type_id = dict()\n",
    "f = open(root_path+dataset+'e2types.txt','w', encoding='utf-8')\n",
    "for e in e_2_type_dict.keys():\n",
    "    types = e_2_type_dict[e]\n",
    "    ids = [type_2_id[type] for type in types]\n",
    "    e_2_type_id[e] = ids\n",
    "    f.writelines(str(e) + '\t' + str(ids) + '\\n')\n",
    "f.close()\n",
    "\n",
    "print(len(type_2_id.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
